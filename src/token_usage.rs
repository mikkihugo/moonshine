//! # Token Usage Tracking for AI Language Models
//!
//! This module provides comprehensive token usage tracking for AI language model interactions,
//! supporting both standard and advanced features like reasoning tokens from Claude-3.5-Sonnet-20241022.
//!
//! ## Features
//! - **Token Counting**: Input, output, and total token tracking
//! - **Provider Tracking**: Which AI provider was used for the request
//! - **Performance Metrics**: Execution time tracking for optimization
//! - **Reasoning Tokens**: Support for advanced reasoning token tracking
//! - **Aggregation**: Combine usage statistics across multiple requests
//!
//! @category ai-integration
//! @safe team
//! @mvp core
//! @complexity low
//! @since 1.0.0

use serde::{Deserialize, Serialize};
use std::ops::Add;

/// Language model usage statistics with comprehensive tracking
///
/// Tracks token consumption, provider information, and performance metrics
/// for AI language model interactions. Supports aggregation across multiple
/// requests for cost analysis and optimization.
///
/// @category ai-struct
/// @safe team
/// @mvp core
/// @complexity low
/// @since 1.0.0
#[derive(Clone, Debug, Default, Serialize, Deserialize)]
pub struct LanguageModelUsageMetrics {
    /// Number of input tokens sent to the model
    pub input_tokens: u32,
    /// Number of output tokens generated by the model
    pub output_tokens: u32,
    /// Total tokens (input + output) for billing calculations
    pub total_tokens: u32,
    /// Advanced reasoning tokens (available on Claude-3.5-Sonnet-20241022+)
    pub reasoning_tokens: Option<u32>,
    /// AI provider name (claude, gpt-4, gemini, etc.)
    pub provider_used: Option<String>,
    /// Execution time in milliseconds for performance tracking
    pub execution_time_ms: Option<u64>,
}

/// Aggregation implementation for combining usage statistics
///
/// Allows combining multiple `LanguageModelUsageMetrics` instances to track cumulative
/// token consumption across multiple AI requests. Useful for session
/// totals and cost analysis.
impl Add for LanguageModelUsageMetrics {
    type Output = LanguageModelUsageMetrics;

    /// Combine two usage instances, summing token counts and preserving metadata
    fn add(self, other: LanguageModelUsageMetrics) -> Self {
        LanguageModelUsageMetrics {
            input_tokens: self.input_tokens + other.input_tokens,
            output_tokens: self.output_tokens + other.output_tokens,
            total_tokens: self.total_tokens + other.total_tokens,
            reasoning_tokens: match (self.reasoning_tokens, other.reasoning_tokens) {
                (Some(a), Some(b)) => Some(a + b),
                (Some(a), None) => Some(a),
                (None, Some(b)) => Some(b),
                (None, None) => None,
            },
            // For aggregation, prefer the first provider or take the other if first is None
            provider_used: self.provider_used.or(other.provider_used),
            // Sum execution times if both present, otherwise take the available one
            execution_time_ms: match (self.execution_time_ms, other.execution_time_ms) {
                (Some(a), Some(b)) => Some(a + b),
                (Some(a), None) => Some(a),
                (None, Some(b)) => Some(b),
                (None, None) => None,
            },
        }
    }
}

impl LanguageModelUsageMetrics {
    /// Creates a new LanguageModelUsageMetrics with specified token counts
    pub fn new(input_tokens: u32, output_tokens: u32) -> Self {
        Self {
            input_tokens,
            output_tokens,
            total_tokens: input_tokens + output_tokens,
            reasoning_tokens: None,
            provider_used: None,
            execution_time_ms: None,
        }
    }

    /// Creates a new LanguageModelUsageMetrics with all fields specified
    pub fn with_all_fields(
        input_tokens: u32,
        output_tokens: u32,
        reasoning_tokens: Option<u32>,
        provider_used: Option<String>,
        execution_time_ms: Option<u64>,
    ) -> Self {
        Self {
            input_tokens,
            output_tokens,
            total_tokens: input_tokens + output_tokens + reasoning_tokens.unwrap_or(0),
            reasoning_tokens,
            provider_used,
            execution_time_ms,
        }
    }

    /// Returns the efficiency ratio (output tokens / input tokens)
    pub fn efficiency_ratio(&self) -> f64 {
        if self.input_tokens == 0 {
            0.0
        } else {
            self.output_tokens as f64 / self.input_tokens as f64
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_lm_usage_default() {
        let usage = LanguageModelUsageMetrics::default();
        assert_eq!(usage.input_tokens, 0);
        assert_eq!(usage.output_tokens, 0);
        assert_eq!(usage.total_tokens, 0);
        assert_eq!(usage.reasoning_tokens, None);
        assert_eq!(usage.provider_used, None);
        assert_eq!(usage.execution_time_ms, None);
    }

    #[test]
    fn test_lm_usage_new() {
        let usage = LanguageModelUsageMetrics::new(100, 50);
        assert_eq!(usage.input_tokens, 100);
        assert_eq!(usage.output_tokens, 50);
        assert_eq!(usage.total_tokens, 150);
        assert_eq!(usage.reasoning_tokens, None);
        assert_eq!(usage.provider_used, None);
        assert_eq!(usage.execution_time_ms, None);
    }

    #[test]
    fn test_lm_usage_with_all_fields() {
        let usage = LanguageModelUsageMetrics::with_all_fields(100, 50, Some(25), Some("claude".to_string()), Some(1500));
        assert_eq!(usage.input_tokens, 100);
        assert_eq!(usage.output_tokens, 50);
        assert_eq!(usage.total_tokens, 175); // 100 + 50 + 25
        assert_eq!(usage.reasoning_tokens, Some(25));
        assert_eq!(usage.provider_used, Some("claude".to_string()));
        assert_eq!(usage.execution_time_ms, Some(1500));
    }

    #[test]
    fn test_lm_usage_add_basic() {
        let usage1 = LanguageModelUsageMetrics::new(100, 50); // total_tokens = 150
        let usage2 = LanguageModelUsageMetrics::new(200, 75); // total_tokens = 275

        let combined = usage1 + usage2;
        assert_eq!(combined.input_tokens, 300);
        assert_eq!(combined.output_tokens, 125);
        assert_eq!(combined.total_tokens, 425); // 150 + 275 = 425 (sum of original totals)
    }

    #[test]
    fn test_lm_usage_add_with_reasoning_tokens() {
        let usage1 = LanguageModelUsageMetrics::with_all_fields(100, 50, Some(10), None, None);
        let usage2 = LanguageModelUsageMetrics::with_all_fields(200, 75, Some(15), None, None);

        let combined = usage1 + usage2;
        assert_eq!(combined.reasoning_tokens, Some(25));
    }

    #[test]
    fn test_lm_usage_add_reasoning_tokens_partial() {
        let usage1 = LanguageModelUsageMetrics::with_all_fields(100, 50, Some(10), None, None);
        let usage2 = LanguageModelUsageMetrics::new(200, 75); // No reasoning tokens

        let combined = usage1 + usage2;
        assert_eq!(combined.reasoning_tokens, Some(10));
    }

    #[test]
    fn test_lm_usage_add_provider_selection() {
        let usage1 = LanguageModelUsageMetrics::with_all_fields(100, 50, None, Some("claude".to_string()), None);
        let usage2 = LanguageModelUsageMetrics::with_all_fields(200, 75, None, Some("gpt4".to_string()), None);

        let combined = usage1 + usage2;
        assert_eq!(combined.provider_used, Some("claude".to_string())); // First provider wins

        // Test with first being None
        let usage3 = LanguageModelUsageMetrics::new(100, 50);
        let usage4 = LanguageModelUsageMetrics::with_all_fields(200, 75, None, Some("gemini".to_string()), None);

        let combined2 = usage3 + usage4;
        assert_eq!(combined2.provider_used, Some("gemini".to_string()));
    }

    #[test]
    fn test_lm_usage_add_execution_time() {
        let usage1 = LanguageModelUsageMetrics::with_all_fields(100, 50, None, None, Some(1000));
        let usage2 = LanguageModelUsageMetrics::with_all_fields(200, 75, None, None, Some(1500));

        let combined = usage1 + usage2;
        assert_eq!(combined.execution_time_ms, Some(2500));

        // Test with one None
        let usage3 = LanguageModelUsageMetrics::new(100, 50);
        let usage4 = LanguageModelUsageMetrics::with_all_fields(200, 75, None, None, Some(800));

        let combined2 = usage3 + usage4;
        assert_eq!(combined2.execution_time_ms, Some(800));
    }

    #[test]
    fn test_efficiency_ratio() {
        let usage1 = LanguageModelUsageMetrics::new(100, 50);
        assert_eq!(usage1.efficiency_ratio(), 0.5);

        let usage2 = LanguageModelUsageMetrics::new(50, 100);
        assert_eq!(usage2.efficiency_ratio(), 2.0);

        let usage3 = LanguageModelUsageMetrics::new(0, 50);
        assert_eq!(usage3.efficiency_ratio(), 0.0); // Avoid division by zero
    }

    #[test]
    fn test_lm_usage_serialization() {
        let usage = LanguageModelUsageMetrics::with_all_fields(100, 50, Some(25), Some("claude".to_string()), Some(1200));

        // Test serialization
        let serialized = serde_json::to_string(&usage).unwrap();
        assert!(serialized.contains("input_tokens"));
        assert!(serialized.contains("claude"));

        // Test deserialization
        let deserialized: LanguageModelUsageMetrics = serde_json::from_str(&serialized).unwrap();
        assert_eq!(deserialized.input_tokens, usage.input_tokens);
        assert_eq!(deserialized.output_tokens, usage.output_tokens);
        assert_eq!(deserialized.reasoning_tokens, usage.reasoning_tokens);
        assert_eq!(deserialized.provider_used, usage.provider_used);
        assert_eq!(deserialized.execution_time_ms, usage.execution_time_ms);
    }

    #[test]
    fn test_lm_usage_clone() {
        let usage = LanguageModelUsageMetrics::with_all_fields(100, 50, Some(10), Some("gpt4".to_string()), Some(900));

        let cloned = usage.clone();
        assert_eq!(cloned.input_tokens, usage.input_tokens);
        assert_eq!(cloned.output_tokens, usage.output_tokens);
        assert_eq!(cloned.reasoning_tokens, usage.reasoning_tokens);
        assert_eq!(cloned.provider_used, usage.provider_used);
        assert_eq!(cloned.execution_time_ms, usage.execution_time_ms);
    }

    #[test]
    fn test_multiple_additions() {
        let usage1 = LanguageModelUsageMetrics::new(100, 50);
        let usage2 = LanguageModelUsageMetrics::new(200, 75);
        let usage3 = LanguageModelUsageMetrics::new(150, 100);

        let combined = usage1 + usage2 + usage3;
        assert_eq!(combined.input_tokens, 450);
        assert_eq!(combined.output_tokens, 225);
    }
}
