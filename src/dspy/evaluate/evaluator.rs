//! # DSPy Evaluator: Trait for Module Performance Assessment
//!
//! This module defines the `Evaluator` trait, which is a fundamental component within the DSPy framework
//! for assessing the performance and quality of DSPy modules. The `Evaluator` trait extends the `Module`
//! trait, allowing any DSPy module to also define how its predictions are measured against ground truth examples.
//!
//! Evaluation is crucial for the DSPy optimization loop, as it provides the feedback signal
//! that drives the self-improvement process of AI models and their associated prompts.
//!
//! @category dspy-evaluate
//! @safe program
//! @mvp core
//! @complexity medium
//! @since 1.0.0

use crate::data::{Example, Prediction};
use crate::dspy::core::Module;
use futures::future::join_all;

/// Defines the interface for evaluating the performance of a DSPy module.
///
/// An `Evaluator` is a `Module` that can also compute a metric for its predictions
/// against a set of examples. This trait is essential for the DSPy optimization process,
/// where modules are iteratively improved based on their evaluation scores.
#[allow(async_fn_in_trait)]
pub trait Evaluator: Module {
  /// The maximum concurrency level for batch processing during evaluation.
  const MAX_CONCURRENCY: usize = 32;
  /// A flag indicating whether to display progress during batch processing.
  const DISPLAY_PROGRESS: bool = true;

  /// Computes a metric score for a single example-prediction pair.
  ///
  /// This method defines how the quality of a module's prediction is measured
  /// against a given example. The metric score is typically a float between 0.0 and 1.0.
  ///
  /// # Arguments
  ///
  /// * `example` - The input `Example` used for the prediction.
  /// * `prediction` - The `Prediction` generated by the module.
  ///
  /// # Returns
  ///
  /// A `f32` representing the metric score.
  async fn metric(&self, example: &Example, prediction: &Prediction) -> f32;

  /// Evaluates the module's performance over a collection of examples.
  ///
  /// This method processes a batch of examples, generates predictions, and then
  /// computes the average metric score across all predictions. It leverages the
  /// `batch` method from the `Module` trait for concurrent processing.
  ///
  /// # Arguments
  ///
  /// * `examples` - A `Vec<Example>` representing the dataset for evaluation.
  ///
  /// # Returns
  ///
  /// A `f32` representing the average metric score across all examples.
  async fn evaluate(&self, examples: Vec<Example>) -> f32 {
    let predictions = match self
      .batch(
        examples.clone(),
        Self::MAX_CONCURRENCY,
        Self::DISPLAY_PROGRESS,
      )
      .await
    {
      Ok(predictions) => predictions,
      Err(_) => {
        // Return 0.0 score if batch prediction fails
        return 0.0;
      }
    };

    let futures: Vec<_> = examples
      .iter()
      .zip(predictions.iter())
      .map(|(example, prediction)| {
        let prediction = prediction.clone();
        async move { self.metric(example, &prediction).await }
      })
      .collect();

    let metrics = join_all(futures).await;
    metrics.iter().sum::<f32>() / examples.len() as f32
  }
}
